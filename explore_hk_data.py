"""
æ¢ç´¢é¦™æ¸¯æ”¿åºœæ•°æ®é›†ï¼Œå¯»æ‰¾é€‚åˆæ—¶é—´åºåˆ—åˆ†æçš„æ•°æ®
"""
import requests
import json
from datetime import datetime

BASE_URL = "https://app.data.gov.hk/v1/historical-archive/list-files"

# é€‚åˆæ—¶é—´åºåˆ—åˆ†æçš„ç±»åˆ«
GOOD_CATEGORIES = [
    ("climate-and-weather", "æ°”è±¡"),
    ("transport", "è¿è¾“"),
    ("finance", "è´¢ç»"),
    ("environment", "ç¯å¢ƒ"),
    ("health", "å«ç”Ÿ"),
    ("population", "äººå£"),
    ("commerce-and-industry", "å·¥å•†ä¸š"),
    ("housing", "æˆ¿å±‹"),
]

# é€‚åˆæ—¶é—´åºåˆ—çš„æ•°æ®æä¾›è€…
GOOD_PROVIDERS = [
    ("hk-hko", "é¦™æ¸¯å¤©æ–‡å°"),
    ("hk-td", "è¿è¾“ç½²"),
    ("hk-censtatd", "æ”¿åºœç»Ÿè®¡å¤„"),
    ("hk-epd", "ç¯å¢ƒä¿æŠ¤ç½²"),
    ("hk-md", "æµ·äº‹å¤„"),
    ("mtr", "é¦™æ¸¯é“è·¯æœ‰é™å…¬å¸"),
    ("hk-hkma", "é¦™æ¸¯é‡‘èç®¡ç†å±€"),
]

def query_api(start, end, category=None, provider=None, format=None, search=None, max_results=100):
    """è°ƒç”¨é¦™æ¸¯æ”¿åºœæ•°æ®API"""
    params = {
        "start": start,
        "end": end,
        "max": max_results,
    }
    if category:
        params["category"] = category
    if provider:
        params["provider"] = provider
    if format:
        params["format"] = format
    if search:
        params["search"] = search
    
    try:
        response = requests.get(BASE_URL, params=params, timeout=30)
        if response.status_code == 200:
            return response.json()
        else:
            print(f"Error: {response.status_code}")
            return None
    except Exception as e:
        print(f"Request error: {e}")
        return None

def explore_categories():
    """æ¢ç´¢å„ä¸ªç±»åˆ«çš„æ•°æ®"""
    print("=" * 80)
    print("æ¢ç´¢é¦™æ¸¯æ”¿åºœæ•°æ®é›† - å¯»æ‰¾é€‚åˆæ—¶é—´åºåˆ—åˆ†æçš„æ•°æ®")
    print("=" * 80)
    
    # è®¾ç½®æ—¶é—´èŒƒå›´ï¼ˆæœ€è¿‘5å¹´çš„æ•°æ®ï¼‰
    start_date = "20190101"
    end_date = "20241231"
    
    all_results = []
    
    # æŒ‰ç±»åˆ«æ¢ç´¢
    for cat_id, cat_name in GOOD_CATEGORIES:
        print(f"\n--- ç±»åˆ«: {cat_name} ({cat_id}) ---")
        result = query_api(start_date, end_date, category=cat_id, max_results=50)
        
        if result and "files" in result:
            files = result["files"]
            total = result.get("resultCount", len(files))
            print(f"  æ‰¾åˆ° {total} ä¸ªæ–‡ä»¶")
            
            # ç»Ÿè®¡æ•°æ®é›†
            datasets = {}
            for f in files:
                ds_name = f.get("dataset-tc", f.get("dataset-en", "Unknown"))
                provider = f.get("provider", "Unknown")
                if ds_name not in datasets:
                    datasets[ds_name] = {
                        "provider": provider,
                        "count": 0,
                        "formats": set(),
                        "sample_url": f.get("url", "")
                    }
                datasets[ds_name]["count"] += 1
                datasets[ds_name]["formats"].add(f.get("url", "").split(".")[-1])
            
            # æ˜¾ç¤ºæ•°æ®é›†ï¼ˆæŒ‰æ–‡ä»¶æ•°é‡æ’åºï¼‰
            sorted_datasets = sorted(datasets.items(), key=lambda x: x[1]["count"], reverse=True)
            for ds_name, info in sorted_datasets[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"    â€¢ {ds_name}")
                print(f"      æä¾›è€…: {info['provider']}, æ–‡ä»¶æ•°: {info['count']}, æ ¼å¼: {info['formats']}")
            
            all_results.extend([(cat_name, ds_name, info) for ds_name, info in datasets.items()])
    
    return all_results

def explore_weather_data():
    """ä¸“é—¨æ¢ç´¢æ°”è±¡æ•°æ®ï¼ˆæœ€é€‚åˆæ—¶é—´åºåˆ—ï¼‰"""
    print("\n" + "=" * 80)
    print("é‡ç‚¹æ¢ç´¢: é¦™æ¸¯å¤©æ–‡å°æ°”è±¡æ•°æ®")
    print("=" * 80)
    
    # æŸ¥è¯¢å¤©æ–‡å°æ•°æ®
    result = query_api("20150101", "20241231", provider="hk-hko", max_results=200)
    
    if result and "files" in result:
        files = result["files"]
        print(f"æ‰¾åˆ° {result.get('resultCount', len(files))} ä¸ªå¤©æ–‡å°æ•°æ®æ–‡ä»¶")
        
        # æŒ‰æ•°æ®é›†åˆ†ç»„
        datasets = {}
        for f in files:
            ds_name = f.get("dataset-tc", f.get("dataset-en", "Unknown"))
            resource = f.get("resource-tc", f.get("resource-en", ""))
            url = f.get("url", "")
            
            if ds_name not in datasets:
                datasets[ds_name] = []
            datasets[ds_name].append({
                "resource": resource,
                "url": url,
                "format": url.split(".")[-1] if "." in url else "unknown"
            })
        
        print(f"\nå…±æœ‰ {len(datasets)} ä¸ªä¸åŒçš„æ•°æ®é›†:")
        for ds_name, files in sorted(datasets.items(), key=lambda x: len(x[1]), reverse=True):
            print(f"\nğŸ“Š {ds_name}")
            print(f"   æ–‡ä»¶æ•°é‡: {len(files)}")
            formats = set(f["format"] for f in files)
            print(f"   æ ¼å¼: {formats}")
            # æ˜¾ç¤ºå‰3ä¸ªèµ„æº
            for f in files[:3]:
                print(f"   - {f['resource'][:50]}..." if len(f['resource']) > 50 else f"   - {f['resource']}")

def explore_transport_data():
    """æ¢ç´¢è¿è¾“æ•°æ®"""
    print("\n" + "=" * 80)
    print("é‡ç‚¹æ¢ç´¢: è¿è¾“æ•°æ®")
    print("=" * 80)
    
    result = query_api("20150101", "20241231", category="transport", max_results=200)
    
    if result and "files" in result:
        files = result["files"]
        print(f"æ‰¾åˆ° {result.get('resultCount', len(files))} ä¸ªè¿è¾“æ•°æ®æ–‡ä»¶")
        
        datasets = {}
        for f in files:
            ds_name = f.get("dataset-tc", f.get("dataset-en", "Unknown"))
            provider = f.get("provider", "Unknown")
            
            if ds_name not in datasets:
                datasets[ds_name] = {"provider": provider, "count": 0, "files": []}
            datasets[ds_name]["count"] += 1
            datasets[ds_name]["files"].append(f)
        
        print(f"\nå…±æœ‰ {len(datasets)} ä¸ªä¸åŒçš„æ•°æ®é›†:")
        for ds_name, info in sorted(datasets.items(), key=lambda x: x[1]["count"], reverse=True)[:10]:
            print(f"\nğŸ“Š {ds_name}")
            print(f"   æä¾›è€…: {info['provider']}, æ–‡ä»¶æ•°: {info['count']}")

def explore_specific_search():
    """æœç´¢ç‰¹å®šå…³é”®è¯çš„æ•°æ®"""
    print("\n" + "=" * 80)
    print("å…³é”®è¯æœç´¢: æ—¶é—´åºåˆ—ç›¸å…³æ•°æ®")
    print("=" * 80)
    
    keywords = ["daily", "monthly", "temperature", "passenger", "traffic", "pollution", "rainfall"]
    
    for keyword in keywords:
        result = query_api("20190101", "20241231", search=keyword, max_results=20)
        if result and "files" in result:
            count = result.get("resultCount", 0)
            print(f"\nğŸ” å…³é”®è¯ '{keyword}': æ‰¾åˆ° {count} ä¸ªç»“æœ")
            
            if count > 0:
                # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
                for f in result["files"][:3]:
                    ds_name = f.get("dataset-tc", f.get("dataset-en", "Unknown"))
                    print(f"   - {ds_name}")

if __name__ == "__main__":
    print("å¼€å§‹æ¢ç´¢é¦™æ¸¯æ”¿åºœæ•°æ®é›†...\n")
    
    # 1. æ¢ç´¢å„ç±»åˆ«
    explore_categories()
    
    # 2. é‡ç‚¹æ¢ç´¢æ°”è±¡æ•°æ®
    explore_weather_data()
    
    # 3. æ¢ç´¢è¿è¾“æ•°æ®
    explore_transport_data()
    
    # 4. å…³é”®è¯æœç´¢
    explore_specific_search()
    
    print("\n" + "=" * 80)
    print("æ¢ç´¢å®Œæˆï¼")
    print("=" * 80)
